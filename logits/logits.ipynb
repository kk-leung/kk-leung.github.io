{
  "cells": [
    {
      "metadata": {
        "_uuid": "57e6b9d2eb90d027551b8761bed35d0deeca0f7c",
        "_cell_guid": "0e83be6a-65f3-47a3-bfe3-063a1c923725"
      },
      "cell_type": "markdown",
      "source": "# Testing Logits Scales\n\n## Introduction\n\nWhen I was learning how to use CNN to tackle the __[Carvana Competition](https://www.kaggle.com/c/carvana-image-masking-challenge)__ on __[Kaggle](http://www.kaggle.com)__, I came across with __[ENet](https://arxiv.org/pdf/1606.02147.pdf)__ that was used for Semantic Segmentation. I skimmed through a __[paper](https://arxiv.org/pdf/1606.02147.pdf)__ and looked at some __[sample code](https://github.com/kwotsin/TensorFlow-ENet/blob/master/enet.py)__ online,  and I started writing my code for the ENet. If x is my input, then z=ENet(x) is the set of logits I used for the calculation of the sigmoid cross entropy. I was not sure if I have made any mistakes in the codes, but using the Xavier initialization of the weights, the output Z are really large - about 1e40 or more. As ENet is composed of many \"bottleneck\" layers, I found out that after each pass of these bottleneck layers, the scale of the output is increased by a factor of 10. Thus, I tried to \"manually\" divide the output by 10 and the ENet finally kinda worked (far from perfectly, but at least the loss function is not NaN anymore and it is decreasing when training.\n\nBecause of this, I would like to investigate the effect of training and performance with different scales of logits using the famous MNIST data. (I cannot find anything online about this topics - but I think this is a very common question.)\n\n***\n## Preliminary Analysis\n\nLet's start with some theoretical analysis. We let\n*  $x$ - the input (`shape = [num_data, 784] or [num_data, 28,28]`). \n*  $z = f(x|\\theta)$ - the logits given parameters $\\theta$ in a given network architecture $f$. (`z.shape = [num_data, 10]`)\n*  $\\hat{y} = softmax(z)$ - our prediction probability. (`shape = [num_data, 10]`)\n*  $y$ - labels. (`shape = [num_data, 10]` after one hot)\n\nThen the loss function is given by\n$$L(x;\\theta)=-\\sum y_i \\log \\hat{y}_i$$\n\nTo reduce the loss, i.e. to train the model, we looked at the derivatives of $L$ with respect to $\\theta$. We first have\n$$\\dfrac{\\partial L}{\\partial z} =y- \\hat{y}$$\n\nThus by chain rules, we have\n$$\\dfrac{\\partial L}{\\partial \\theta} =(y - \\hat{y}) \\dfrac{\\partial }{\\partial \\theta}\\,f(x;\\theta)$$\n\n(Note that all of these are indeed vectors and matrices)\n\nNow we are trying to scale the logits, i.e.  let\n* $\\hat{\\tilde{y}} = softmax(\\alpha z)$ for some $\\alpha > 0$. \n\nWe  then have\n$$\\dfrac{\\partial \\tilde{L}}{\\partial z} =\\alpha(y- \\hat{\\tilde{y}})$$\n\nThis means \n$$\\dfrac{\\partial \\tilde{L}}{\\partial \\theta} =\\alpha(y - \\hat{\\tilde{y}}) \\dfrac{\\partial }{\\partial \\theta}\\,f(x;\\theta)$$\n\nThis means that **if we scale our logits by a factor of $\\alpha$, the gradient is also (apparently) scaled by a factor of $\\alpha$**. Of course it is *not that simple*, because if we scaled the logits, our probability vectors $\\hat{y}$ changed and thus our losses also changed. It is very difficult to analyze how the training would turn out, but we can *try* to ignore the changes in our losses and just think that the gradient is just scaled by a factor of $\\alpha$. Since we used the gradient for our learning, **scaling the logits by a factor of $\\alpha$ is similar to scaling our learning rate by a factor of $\\alpha$**, ignoring the (huge) effects of the changes in the loss function."
    },
    {
      "metadata": {
        "_uuid": "eab20e8334b484d3e204aa45a241c41bdaf336de",
        "_cell_guid": "a40529f1-5b10-42cc-be76-60c1899f5d16"
      },
      "cell_type": "markdown",
      "source": "## Demo\nThus we can do some experiments to see how the scaling of the logits would affect our training.\n\nWe are doing the followings,\n-  We are using 98% of our data as training set and 2% of our data \n-  We are investigating several archictecture including a fully connected net and three CNNs.\n-  We fix the number of epochs to be 5 (or 10) \n-  We are using simple gradient descent optimizer.\n-  Minibatch Size for Gradient Descent is 1024.\n-  Our base learning rate is 0.01 when it is not adjusted by the scale. \n-  We will be keeping track of the standard deviation of the logits\n-  We are fixing a random state\n\nNote that this notebook is run on __[Kaggle](www.kaggle.com)__ Kernel."
    },
    {
      "metadata": {
        "_uuid": "3f6191e3dd6751700128149e5fe6592a82c64560",
        "collapsed": true,
        "_cell_guid": "640cdbbb-6a90-4a9d-954b-fe8d8d13e987",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "43e007b784d83bbf466c3e42fb81a14b38b2fb23",
        "_cell_guid": "f86eae48-2737-4244-b7a8-0c475ec7c5fd",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('../input/train.csv')\ndf.head()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0       0    ...            0         0         0         0         0   \n1       0    ...            0         0         0         0         0   \n2       0    ...            0         0         0         0         0   \n3       0    ...            0         0         0         0         0   \n4       0    ...            0         0         0         0         0   \n\n   pixel779  pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0         0  \n1         0         0         0         0         0  \n2         0         0         0         0         0  \n3         0         0         0         0         0  \n4         0         0         0         0         0  \n\n[5 rows x 785 columns]",
            "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "bb9c505b9624c0b588843055a18cd740e3a330dd",
        "_cell_guid": "9dc5a0ed-d008-489f-bdc1-88a35c2da091",
        "trusted": true
      },
      "cell_type": "code",
      "source": "Xtrain, Xtest, ytrain, ytest = train_test_split(df.iloc[:,1:],df.iloc[:,0],train_size=0.98, test_size=0.02, random_state=0)\nXtrain=np.array(Xtrain).reshape(-1,28,28,1)\nXtest=np.array(Xtest).reshape(-1,28,28,1)\nenc = OneHotEncoder()\nytrain= enc.fit_transform(np.array(ytrain).reshape(-1,1)).toarray()\nytest= enc.transform(np.array(ytest).reshape(-1,1)).toarray()\nprint(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(41160, 28, 28, 1) (840, 28, 28, 1) (41160, 10) (840, 10)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "01889ec80d517f72e13744e83f4c89e5d21afcad",
        "collapsed": true,
        "_cell_guid": "3d4c5fa2-7338-4390-9637-ebdfcc1890a4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Several network architecture. the input x is the input placeholder and return a logit tensor.\ndef cnn1(x):\n    net = tf.layers.conv2d(x,8,[5, 5],strides=(1,1),padding=\"valid\",activation=tf.tanh)\n    net = tf.layers.max_pooling2d(inputs=net, pool_size=[2, 2], strides=2)\n    net = tf.layers.conv2d(net,32, [5,5], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    net = tf.layers.max_pooling2d(inputs=net, pool_size=[2, 2], strides=2)\n    net = tf.layers.conv2d(net,1, [1,1], strides=(1,1), padding=\"same\", activation=None)\n    n_train = tf.shape(net)[0]\n    net = tf.reshape(net, [n_train, 16])\n    return tf.contrib.layers.fully_connected(net, 10, activation_fn=None)\n\ndef fc(x):\n    n_train = tf.shape(x)[0]\n    net = tf.reshape(x, [n_train, 784])\n    net = tf.contrib.layers.fully_connected(net, 196, activation_fn=tf.nn.relu)\n    net = tf.contrib.layers.fully_connected(net, 49, activation_fn=tf.nn.relu)\n    return tf.contrib.layers.fully_connected(net, 10, activation_fn=None)\n\ndef cnn2(x):\n    net = tf.layers.conv2d(x , 4, [5,5], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    net = tf.layers.conv2d(net,4, [5,5], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    net = tf.layers.conv2d(net,4, [5,5], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    net = tf.layers.conv2d(net,4, [5,5], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    net = tf.layers.conv2d(net,4, [5,5], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    net = tf.layers.conv2d(net,4, [5,5], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    net = tf.layers.conv2d(net,1, [1,1], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    n_train = tf.shape(net)[0]\n    net = tf.reshape(net, [n_train, 16])\n    return tf.contrib.layers.fully_connected(net, 10, activation_fn=None)\n\ndef cnn3(x):\n    net = tf.layers.conv2d(x,16,[3, 3],strides=(1,1),padding=\"valid\",activation=tf.tanh)\n    net = tf.layers.max_pooling2d(inputs=net, pool_size=[2, 2], strides=2)\n    net = tf.layers.conv2d(net,64, [4,4], strides=(1,1), padding=\"valid\", activation=tf.tanh)\n    net = tf.layers.max_pooling2d(inputs=net, pool_size=[2, 2], strides=2)\n    net = tf.layers.conv2d(net,1, [1,1], strides=(1,1), padding=\"same\", activation=None)\n    n_train = tf.shape(net)[0]\n    net = tf.reshape(net, [n_train, 25])\n    return tf.contrib.layers.fully_connected(net, 10, activation_fn=None)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed018241ffece02c05360bcd2e2f33928542c5f8",
        "collapsed": true,
        "_cell_guid": "57831c05-88c0-437b-9907-68817e7805f3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def runnet(Xtrain, Xtest, ytrain, ytest, func,epochs = 5,scale=1, base_lr = 0.001, learning_rate_adjusted = False, seed=None):\n    if func not in [cnn1,cnn2,cnn3,fc]:\n        print('Input Function Incorrect!')\n        return\n    numlist=np.array([0,1,2,3,4,5,6,7,8,9])\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    x = tf.placeholder(tf.float32, shape=[None, 28,28,1])\n    y = tf.placeholder(tf.float32, shape = [None, 10])\n    lr = tf.placeholder(tf.float32, shape = [])\n    out = func(x) * scale\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))\n    train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n    bs = 1024\n    acclist=[]\n    stdlist=[]\n    #trainlist=[]\n    #testlist=[]\n    if learning_rate_adjusted:\n        rate = base_lr / scale \n    else:\n        rate = base_lr\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        er=sess.run(loss, feed_dict={x:Xtrain[:1], y:ytrain[:1], lr: 0})\n        print('Running: Scale={}, BaseLR={}, LR_Adjusted={}, Seed={}'.format(scale, base_lr, learning_rate_adjusted, seed))\n        print('Initial Logits StDev: ',np.std(sess.run(out, feed_dict={x:Xtest})))\n        for i in range(epochs):\n            tic = time.time()\n            for j in range(len(Xtrain)//bs):\n                _, er=sess.run([train_step, loss], feed_dict={x:Xtrain[j*bs:(j+1)*bs], y:ytrain[j*bs:(j+1)*bs], lr: rate})\n                ertest, pred = sess.run([loss,out], feed_dict={x:Xtest, y:ytest})\n                stdev = np.std(pred)\n                pred=np.argmax(pred, axis=1)\n                acc=(pred==np.dot(ytest, numlist)).sum()/len(ytest)*100\n\n                if j%3==0:\n                    #print('Training Error: {:8.4f}   Test Error: {:8.4f}   Accuracy: {:5.2f}%'.format(er, ertest, acc))\n                    acclist.append(acc)\n                    stdlist.append(stdev)\n                    #trainlist.append(er)\n                    #testlist.append(ertest)\n            toc = time.time()\n            print('Epoch {}   Training Error: {:8.4f}   Test Error: {:8.4f}   Accuracy: {:5.2f}%   Time: {:6.2f}s'.format(i+1,er, ertest, acc, toc- tic))\n            tic = toc\n        #print(np.std(sess.run(out, feed_dict={x:Xtest})))\n    return acclist, stdlist#, trainlist, testlist\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "85480a29413b88d95b9870cb90003a98661554fd",
        "_cell_guid": "aa17b98c-3ce5-4efa-a68c-089052e2c9e0",
        "scrolled": true,
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#fun, optimizer, baselr, Adj, SD, Scales Epochs\nbase_lr=0.01\nscalelist = [0.1,0.3,1,3,10] #A\n#scalelist = [0.01,0.1,1,10,100] #B\n#scalelist = [1e-8,1e-6,1e-4,1e-2,1] #C\n#scalelist = [1,100,1e4,1e6,1e8] #D\nep=10\nadj=True\nsd=0\nACC = []\nSTD = []\nfname='C3AD1e-2TTA10'\nfor scale in scalelist:\n    acclist, stdlist = runnet(Xtrain, Xtest, ytrain, ytest, cnn3, base_lr=base_lr,epochs = ep,scale=scale,learning_rate_adjusted = adj,seed=sd)\n    ACC.append(acclist)\n    STD.append(stdlist)\nplt.title('CNN1 - Accuracy\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))\nfor i in range(len(ACC)):\n    plt.plot(ACC[i], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-ACC.jpg', dpi=100)\nplt.show()\nplt.title('CNN1 - Logits StDev\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))   \nfor i in range(len(STD)):\n    plt.plot(STD[i][20:], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-STD.jpg', dpi=100)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c0d98c3585f908cc48c5b768bfdb6f5f1f56f50",
        "collapsed": true,
        "_cell_guid": "94b8626d-3bff-4b4b-8bc2-32cc9ea7d0f5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "base_lr=0.01\n#scalelist = [0.1,0.3,1,3,10] #A\nscalelist = [0.01,0.1,1,10,100] #B\n#scalelist = [1e-8,1e-6,1e-4,1e-2,1] #C\n#scalelist = [1,100,1e4,1e6,1e8] #D\nep=10\nadj=True\nsd=0\nACC = []\nSTD = []\nfname='C3AD1e-2TTB10'\nfor scale in scalelist:\n    acclist, stdlist = runnet(Xtrain, Xtest, ytrain, ytest, cnn3, base_lr=base_lr,epochs = ep,scale=scale,learning_rate_adjusted = adj,seed=sd)\n    ACC.append(acclist)\n    STD.append(stdlist)\nplt.title('CNN1 - Accuracy\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))\nfor i in range(len(ACC)):\n    plt.plot(ACC[i], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-ACC.jpg', dpi=100)\nplt.show()\nplt.title('CNN1 - Logits StDev\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))   \nfor i in range(len(STD)):\n    plt.plot(STD[i][20:], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-STD.jpg', dpi=100)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4337ea2219ebc2c7463f41f2cce06dfb1af38ee9",
        "_cell_guid": "adb99fd2-ff6a-4bd7-995c-52e3956f0bfa"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_cell_guid": "47630962-eb39-400e-baf6-26217527a802",
        "collapsed": true,
        "_uuid": "23776089b2ad5fb18fe3da31c9d04a47bf1581d5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "base_lr=0.001\nscalelist = [0.1,0.3,1,3,10] #A\n#scalelist = [0.01,0.1,1,10,100] #B\n#scalelist = [1e-8,1e-6,1e-4,1e-2,1] #C\n#scalelist = [1,100,1e4,1e6,1e8] #D\nep=10\nadj=True\nsd=0\nACC = []\nSTD = []\nfname='C3AD1e-3TTA10'\nfor scale in scalelist:\n    acclist, stdlist = runnet(Xtrain, Xtest, ytrain, ytest, cnn3, base_lr=base_lr,epochs = ep,scale=scale,learning_rate_adjusted = adj,seed=sd)\n    ACC.append(acclist)\n    STD.append(stdlist)\nplt.title('CNN1 - Accuracy\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))\nfor i in range(len(ACC)):\n    plt.plot(ACC[i], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-ACC.jpg', dpi=100)\nplt.show()\nplt.title('CNN1 - Logits StDev\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))   \nfor i in range(len(STD)):\n    plt.plot(STD[i][20:], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-STD.jpg', dpi=100)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5678d66fd2ee077e6b4e1dea0d7e5d74ccfe4fb8",
        "collapsed": true,
        "_cell_guid": "845eecf7-6086-4da4-b8fb-a84d93e737cf",
        "trusted": false
      },
      "cell_type": "code",
      "source": "base_lr=0.01\nscalelist = [0.1,0.3,1,3,10] #A\n#scalelist = [0.01,0.1,1,10,100] #B\n#scalelist = [1e-8,1e-6,1e-4,1e-2,1] #C\n#scalelist = [1,100,1e4,1e6,1e8] #D\nep=10\nadj=False\nsd=0\nACC = []\nSTD = []\nfname='C3AD1e-2FTA10'\nfor scale in scalelist:\n    acclist, stdlist = runnet(Xtrain, Xtest, ytrain, ytest, cnn3, base_lr=base_lr,epochs = ep,scale=scale,learning_rate_adjusted = adj,seed=sd)\n    ACC.append(acclist)\n    STD.append(stdlist)\nplt.title('CNN1 - Accuracy\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))\nfor i in range(len(ACC)):\n    plt.plot(ACC[i], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-ACC.jpg', dpi=100)\nplt.show()\nplt.title('CNN1 - Logits StDev\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))   \nfor i in range(len(STD)):\n    plt.plot(STD[i][20:], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-STD.jpg', dpi=100)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e22804d3-a23f-4b48-9b13-0c97d8f16db1",
        "collapsed": true,
        "_uuid": "a36f2e72d809807df23f62a9ec78b73404b938eb",
        "trusted": false
      },
      "cell_type": "code",
      "source": "base_lr=0.01\nscalelist = [0.1,0.3,1,3,10] #A\n#scalelist = [0.01,0.1,1,10,100] #B\n#scalelist = [1e-8,1e-6,1e-4,1e-2,1] #C\n#scalelist = [1,100,1e4,1e6,1e8] #D\nep=10\nadj=True\nsd=None\nACC = []\nSTD = []\nfname='C3AD1e-2TFA10'\nfor scale in scalelist:\n    acclist, stdlist = runnet(Xtrain, Xtest, ytrain, ytest, cnn3, base_lr=base_lr,epochs = ep,scale=scale,learning_rate_adjusted = adj,seed=sd)\n    ACC.append(acclist)\n    STD.append(stdlist)\nplt.title('CNN1 - Accuracy\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))\nfor i in range(len(ACC)):\n    plt.plot(ACC[i], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-ACC.jpg', dpi=100)\nplt.show()\nplt.title('CNN1 - Logits StDev\\nBaseLR={}, LR Adjusted={}, Epochs={}, Seed={}'.format(base_lr,adj,ep,sd))   \nfor i in range(len(STD)):\n    plt.plot(STD[i][20:], label='Scale ='+str(scalelist[i]))\nplt.legend()\nplt.savefig(fname+'-STD.jpg', dpi=100)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c0a8bf03a223475e2568d448c5c12cb8e6e810e",
        "collapsed": true,
        "_cell_guid": "b55da48c-5bc7-41bc-a73a-e412d8cf12e8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def generatemarkdown():\n    import os\n    lis=pd.Series(os.listdir('../working'))\n    a=lis[lis.apply(lambda x: True if len(x)>4 and x[-4:]=='.jpg' and x[:4]=='C1AD' else False)]\n    a=a.sort_values()\n    for name in a:\n        print('<img src=\\''+name+'\\'>')\n    \ngeneratemarkdown()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9afb9f7e87e5e11982dd5393fdb035c75050050e",
        "collapsed": true,
        "_cell_guid": "a3cbceac-b41b-49cc-a154-971b7e0ad993",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from subprocess import check_output\nprint(check_output([\"ls\", \"../working\"]).decode(\"utf8\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "98d82c6778d8533f17902fca76b66bad326c343c",
        "_cell_guid": "aae35321-402d-407a-8144-720e6da616ce"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "4dc3f1e97f89f5866e61177b35411d6991418ebc",
        "_cell_guid": "5043dd3e-8525-4758-97f5-7515d2c5dbb5"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "ab833b8c434fcc7dc9f85d0a04acc1301b5807f5",
        "_cell_guid": "ee694afb-f172-4a58-88cd-4476a9ae634a"
      },
      "cell_type": "markdown",
      "source": "<img src='FCAD1e-3TFA50-ACC.jpg'>\n<img src='FCAD1e-3TFA50-STD.jpg'>\n<img src='FCAD1e-3FTA50-ACC.jpg'>\n<img src='FCAD1e-3FTA50-STD.jpg'>"
    },
    {
      "metadata": {
        "_uuid": "380259de6d1f1242d01d2dd31d81752fb6aa17c7",
        "collapsed": true,
        "_cell_guid": "00789683-8c27-42b9-9ab5-19f8f46044a5"
      },
      "cell_type": "markdown",
      "source": "<img src='C1AD1e-1TTA10-ACC.jpg'>\n<img src='C1AD1e-1TTA10-STD.jpg'>\n<img src='C1AD1e-2TFA10-ACC.jpg'>\n<img src='C1AD1e-2TFA10-STD.jpg'>\n<img src='C1AD1e-2TTA10-ACC.jpg'>\n<img src='C1AD1e-2TTA10-STD.jpg'>\n<img src='C1AD1e-3FTA10-ACC.jpg'>\n<img src='C1AD1e-3FTA10-STD.jpg'>\n<img src='C1AD1e-3TTA10-ACC.jpg'>\n<img src='C1AD1e-3TTA10-STD.jpg'>\n<img src='C1AD1e-3TTB10-ACC.jpg'>\n<img src='C1AD1e-3TTB10-STD.jpg'>\n<img src='C1AD1e-4FTD10-ACC.jpg'>\n<img src='C1AD1e-4FTD10-STD.jpg'>\n<img src='C1AD1e-4TTA10-ACC.jpg'>\n<img src='C1AD1e-4TTA10-STD.jpg'>"
    },
    {
      "metadata": {
        "_uuid": "93c4502f426c3470a7673d1a26bac1cc0ff47b0b",
        "collapsed": true,
        "_cell_guid": "27504656-bff6-4f3a-bdcd-cf7e9a8a9b22",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}